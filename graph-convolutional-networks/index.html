<!DOCTYPE HTML>
<!--
	Spatial by TEMPLATED
	templated.co @templatedco
	Released for free under the Creative Commons Attribution 3.0 license (templated.co/license)
-->
<html>
	<head>
		<title>Graph Convolutional Networks | Thomas Kipf | University of Amsterdam</title>
		<meta http-equiv="content-type" content="text/html; charset=utf-8" />
		<meta name="description" content="" />
		<meta name="keywords" content="" />
		<meta name="twitter:card" content="summary_large_image">
		<meta name="twitter:site" content="@thomaskipf">
		<meta name="twitter:title" content="How powerful are Graph Convolutional Networks?">
		<meta name="twitter:description" content="Many important real-world datasets come in the form of graphs or networks: social networks, knowledge graphs, protein-interaction networks, the World Wide Web, etc. (just to name a few). Yet, until recently, very little attention has been devoted to the generalization of neural...">
		<meta name="twitter:creator" content="@thomaskipf">
		<meta name="twitter:domain" content="http://tkipf.github.io/graph-convolutional-networks/">
		<meta name="twitter:image" content="http://tkipf.github.io/graph-convolutional-networks/images/gcn_web.png">
		<meta property="og:title" content="How powerful are Graph Convolutional Networks?" />
		<meta property="og:description" content="Many important real-world datasets come in the form of graphs or networks: social networks, knowledge graphs, protein-interaction networks, the World Wide Web, etc. (just to name a few). Yet, until recently, very little attention has been devoted to the generalization of neural..." />
		<meta name="og:image" content="http://tkipf.github.io/graph-convolutional-networks/images/gcn_web.png">
		<meta name="og:url" content="http://tkipf.github.io/graph-convolutional-networks/">

		<noscript>
			<link rel="stylesheet" href="/css/style.css" />
			<link rel="stylesheet" href="/css/skel.css" />
			<link rel="stylesheet" href="/css/style-xlarge.css" />
		</noscript>
		<link rel="stylesheet" type="text/css"
		href="https://fonts.googleapis.com/css?family=Raleway:400,700">
		<link rel="stylesheet" type="text/css"
		href="https://fonts.googleapis.com/css?family=Open+Sans">
		<link rel="stylesheet" href="/css/font-awesome.min.css">
		<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
		<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
		<!--[if lte IE 8]><script src="../js/html5shiv.js"></script><![endif]-->
		<script src="/js/jquery.min.js"></script>
		<script src="/js/skel.min.js"></script>
		<script src="/js/skel-layers.min.js"></script>
		<script src="/js/init.js"></script> <!-- NOTE: Pulls CSS files from static server -->
		<script src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
		<script>
			$(document).ready(function(){
				$('video').click( function(){
					if (this.paused) {
			        this.play();
			    } else {
			        this.pause();
			    }
			    return false;
				});
			});
		</script>
	</head>
	<body class="landing">

		<div id="fb-root"></div>
		<script>(function(d, s, id) {
		  var js, fjs = d.getElementsByTagName(s)[0];
		  if (d.getElementById(id)) return;
		  js = d.createElement(s); js.id = id;
		  js.src = "//connect.facebook.net/en_US/sdk.js#xfbml=1&version=v2.7";
		  fjs.parentNode.insertBefore(js, fjs);
		}(document, 'script', 'facebook-jssdk'));</script>
		<script async src="//platform.twitter.com/widgets.js" charset="utf-8"></script>
		<!-- Newsharecounts twitter counter (currently doesn't count re-tweets - bug?)
		<script type="text/javascript" src="//newsharecounts.s3-us-west-2.amazonaws.com/nsc.js"></script><script type="text/javascript">window.newShareCountsAuto="smart";</script>
		-->
		<!-- Opensharecount twitter counter -->
		<script type="text/javascript" src="//opensharecount.com/bubble.js"></script>

		<!-- Header -->
			<header id="header">
				<ul class="icons">
					<li>
						<a href="https://scholar.google.com/citations?user=83HL5FwAAAAJ" class="icon fa-graduation-cap"></a>
					</li>
					<li>
						<a href="https://github.com/tkipf" class="icon fa-github"></a>
					</li>
					<li>
						<a href="https://twitter.com/thomaskipf" class="icon fa-twitter"></a>
					</li>
					<li>
						<a href="https://www.linkedin.com/in/thomas-kipf-6b260410a" class="icon fa-linkedin"></a>
					</li>
				</ul>
				<nav id="nav">
					<ul>
						<li><a href="/">Home</a></li>
						<li><a href="/#two">Publications</a></li>
						<li><a href="/graph-convolutional-networks/" class="active">Blog</a></li>
						<li><a href="/graph-convolutional-networks/#footer">Contact</a></li>
					</ul>
				</nav>
			</header>

			<!-- One -->
				<section id="one" class="wrapper style1 special blogwrapper">
					<div class="container 75%">
						<header class="major blogheader">
							<h2>Graph Convolutional Networks</h2>
							<p>Thomas Kipf, 30 September 2016</p>
						</header>
						<div class="align-left blog">
							<div class="figure">
							<img src="images/gcn_web.png" alt="Multi-layer Graph Convolutional Network (GCN) with first-order filters." />
							<p class="caption">Multi-layer Graph Convolutional Network (GCN) with first-order filters.</p>
							<div class="social_buttons">
							<span class="twitter_button"><a href="https://twitter.com/share" class="twitter-share-button" data-text="How powerful are Graph Convolutional Networks? - An introduction to neural networks on graphs" data-url="http://tkipf.github.io/graph-convolutional-networks/" data-via="thomaskipf" data-lang="en" data-show-count="false">Tweet</a> <a href="http://leadstories.com/opensharecount" target="_blank" class="osc-counter" data-dir="left"  data-url="tkipf.github.io/graph-convolutional-networks/" title="Powered by Lead Stories' OpenShareCount">0</a>
</span>
							<span class="facebook_button"><div class="fb-share-button" data-href="http://tkipf.github.io/graph-convolutional-networks/" data-layout="button_count" data-size="small" data-mobile-iframe="false"><a class="fb-xfbml-parse-ignore" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Ftkipf.github.io%graph-convolutional-networks%2F&amp;src=sdkpreparse">Share</a></div></span>
							</div>
							</div>
							<h3 id="overview">Overview</h3>
							<p>Many important real-world datasets come in the form of graphs or networks: social networks, knowledge graphs, protein-interaction networks, the World Wide Web, etc. (just to name a few). Yet, until recently, very little attention has been devoted to the generalization of neural network models to such structured datasets.</p>
							<p>In the last couple of years, a number of papers re-visited this problem of generalizing neural networks to work on arbitrarily structured graphs (<a target="_blank" href="http://arxiv.org/abs/1312.6203">Bruna et al.</a>, ICLR 2014; <a target="_blank" href="http://arxiv.org/abs/1506.05163">Henaff et al.</a>, 2015; <a target="_blank" href="http://papers.nips.cc/paper/5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints">Duvenaud et al.</a>, NIPS 2015; <a target="_blank" href="https://arxiv.org/abs/1511.05493">Li et al.</a>, ICLR 2016; <a target="_blank" href="https://arxiv.org/abs/1606.09375">Defferrard et al.</a>, NIPS 2016; <a target="_blank" href="http://arxiv.org/abs/1609.02907">Kipf &amp; Welling</a>, ICLR 2017), some of them now achieving very promising results in domains that have previously been dominated by, e.g., kernel-based methods, graph-based regularization techniques and others.</p>
							<p>In this post, I will give a brief overview of recent developments in this field and point out strengths and drawbacks of various approaches. The discussion here will mainly focus on two recent papers:</p>
							<ul>
							<li>Kipf &amp; Welling (ICLR 2017), <a target="_blank" href="http://arxiv.org/abs/1609.02907">Semi-Supervised Classification with Graph Convolutional Networks</a> (disclaimer: I'm the first author)</li>
							<li>Defferrard et al. (NIPS 2016), <a target="_blank" href="https://arxiv.org/abs/1606.09375">Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering</a></li>
							</ul>
							<p>and a review/discussion post by Ferenc Huszar: <a target="_blank" href="http://www.inference.vc/how-powerful-are-graph-convolutions-review-of-kipf-welling-2016-2/">How powerful are Graph Convolutions?</a> that discusses some limitations of these kinds of models. I wrote a short comment on Ferenc's review <a href="#the-issue-with-regular-graphs">here</a> (at the very end of this post).</p>
							<h3 id="outline">Outline</h3>
							<ul>
							<li>Short introduction to neural network models on graphs</li>
							<li>Spectral graph convolutions and <em>Graph Convolutional Networks</em> (GCNs)</li>
							<li>Demo: Graph embeddings with a simple 1st-order GCN model</li>
							<li>GCNs as differentiable generalization of the <em>Weisfeiler-Lehman</em> algorithm</li>
							</ul>
							<p>If you're already familiar with GCNs and related methods, you might want to jump directly to <a href="#gcns-part-iii-embedding-the-karate-club-network">Embedding the karate club network</a>.</p>
							<h3 id="how-powerful-are-graph-convolutional-networks">How powerful are Graph Convolutional Networks?</h3>
							<h4 id="recent-literature">Recent literature</h4>
							<p>Generalizing well-established neural models like RNNs or CNNs to work on arbitrarily structured graphs is a challenging problem. Some recent papers introduce problem-specific specialized architectures (e.g. <a target="_blank" href="http://papers.nips.cc/paper/5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints">Duvenaud et al.</a>, NIPS 2015; <a target="_blank" href="https://arxiv.org/abs/1511.05493">Li et al.</a>, ICLR 2016; <a target="_blank" href="https://arxiv.org/abs/1511.05298">Jain et al.</a>, CVPR 2016), others make use of graph convolutions known from spectral graph theory<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> (<a target="_blank" href="http://arxiv.org/abs/1312.6203">Bruna et al.</a>, ICLR 2014; <a target="_blank" href="http://arxiv.org/abs/1506.05163">Henaff et al.</a>, 2015) to define parameterized filters that are used in a multi-layer neural network model, akin to &quot;classical&quot; CNNs that we know and love.</p>
							<p>More recent work focuses on bridging the gap between fast heuristics and the slow<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>, but somewhat more principled, spectral approach. <a target="_blank" href="https://arxiv.org/abs/1606.09375">Defferrard et al.</a> (NIPS 2016) approximate smooth filters in the spectral domain using Chebyshev polynomials with free parameters that are learned in a neural network-like model. They achieve convincing results on regular domains (like MNIST), closely approaching those of a simple 2D CNN model.</p>
							<p>In <a target="_blank" href="http://arxiv.org/abs/1609.02907">Kipf &amp; Welling</a> (ICLR 2017), we take a somewhat similar approach and start from the framework of spectral graph convolutions, yet introduce simplifications (we will get to those later in the post) that in many cases allow both for significantly faster training times and higher predictive accuracy, reaching state-of-the-art classification results on a number of benchmark graph datasets.</p>
							<h4 id="gcns-part-i-definitions">GCNs Part I: Definitions</h4>
							<p>Currently, most graph neural network models have a somewhat universal architecture in common. I will refer to these models as <em>Graph Convolutional Networks</em> (GCNs); convolutional, because filter parameters are typically shared over all locations in the graph (or a subset thereof as in <a target="_blank" href="http://papers.nips.cc/paper/5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints">Duvenaud et al.</a>, NIPS 2015).</p>
							<p>For these models, the goal is then to learn a function of signals/features on a graph <span class="math inline">\(\mathcal{G}=(\mathcal{V}, \mathcal{E})\)</span> which takes as input:</p>
							<ul>
							<li>A feature description <span class="math inline">\(x_i\)</span> for every node <span class="math inline">\(i\)</span>; summarized in a <span class="math inline">\(N\times D\)</span> feature matrix <span class="math inline">\(X\)</span> (<span class="math inline">\(N\)</span>: number of nodes, <span class="math inline">\(D\)</span>: number of input features)</li>
							<li>A representative description of the graph structure in matrix form; typically in the form of an adjacency matrix <span class="math inline">\(A\)</span> (or some function thereof)</li>
							</ul>
							<p>and produces a node-level output <span class="math inline">\(Z\)</span> (an <span class="math inline">\(N\times F\)</span> feature matrix, where <span class="math inline">\(F\)</span> is the number of output features per node). Graph-level outputs can be modeled by introducing some form of pooling operation (see, e.g. <a target="_blank" href="http://papers.nips.cc/paper/5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints">Duvenaud et al.</a>, NIPS 2015).</p>
							<p>Every neural network layer can then be written as a non-linear function <span class="math display">\[ H^{(l+1)} = f(H^{(l)}, A) \, ,\]</span> with <span class="math inline">\(H^{(0)} = X\)</span> and <span class="math inline">\(H^{(L)} = Z\)</span> (or <span class="math inline">\(z\)</span> for graph-level outputs), <span class="math inline">\(L\)</span> being the number of layers. The specific models then differ only in how <span class="math inline">\(f(\cdot, \cdot)\)</span> is chosen and parameterized.</p>
							<h4 id="gcns-part-ii-a-simple-example">GCNs Part II: A simple example</h4>
							<p>As an example, let's consider the following very simple form of a layer-wise propagation rule:</p>
							<p><span class="math display">\[f(H^{(l)}, A) = \sigma\left( AH^{(l)}W^{(l)}\right) \, ,\]</span></p>
							<p>where <span class="math inline">\(W^{(l)}\)</span> is a weight matrix for the <span class="math inline">\(l\)</span>-th neural network layer and <span class="math inline">\(\sigma(\cdot)\)</span> is a non-linear activation function like the <span class="math inline">\(\text{ReLU}\)</span>. Despite its simplicity this model is already quite powerful (we'll come to that in a moment).</p>
							<p>But first, let us address two limitations of this simple model: multiplication with <span class="math inline">\(A\)</span> means that, for every node, we sum up all the feature vectors of all neighboring nodes but not the node itself (unless there are self-loops in the graph). We can &quot;fix&quot; this by enforcing self-loops in the graph: we simply add the identity matrix to <span class="math inline">\(A\)</span>.</p>
							<p>The second major limitation is that <span class="math inline">\(A\)</span> is typically not normalized and therefore the multiplication with <span class="math inline">\(A\)</span> will completely change the scale of the feature vectors (we can understand that by looking at the eigenvalues of <span class="math inline">\(A\)</span>). Normalizing <span class="math inline">\(A\)</span> such that all rows sum to one, i.e. <span class="math inline">\(D^{-1}A\)</span>, where <span class="math inline">\(D\)</span> is the diagonal node degree matrix, gets rid of this problem. Multiplying with <span class="math inline">\(D^{-1}A\)</span> now corresponds to taking the average of neighboring node features. In practice, dynamics get more interesting when we use a symmetric normalization, i.e. <span class="math inline">\(D^{-\frac{1}{2}}AD^{-\frac{1}{2}}\)</span> (as this no longer amounts to mere averaging of neighboring nodes). Combining these two tricks, we essentially arrive at the propagation rule introduced in <a target="_blank" href="http://arxiv.org/abs/1609.02907">Kipf &amp; Welling</a> (ICLR 2017):</p>
							<p><span class="math display">\[f(H^{(l)}, A) = \sigma\left( \hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}H^{(l)}W^{(l)}\right) \, ,\]</span></p>
							<p>with <span class="math inline">\(\hat{A} = A + I\)</span>, where <span class="math inline">\(I\)</span> is the identity matrix and <span class="math inline">\(\hat{D}\)</span> is the diagonal node degree matrix of <span class="math inline">\(\hat{A}\)</span>.</p>
							<p>In the next section, we will take a closer look at how this type of model operates on a very simple example graph: Zachary's karate club network (make sure to check out the <a target="_blank" href="https://en.wikipedia.org/wiki/Zachary%27s_karate_club">Wikipedia article</a>!).</p>
							<h4 id="gcns-part-iii-embedding-the-karate-club-network">GCNs Part III: Embedding the karate club network</h4>
							<div class="figure">
							<img src="images/karate.png" alt="Karate club graph, colors denote communities obtained via modularity-based clustering (Brandes et al., 2008)." />
							<p class="caption">Karate club graph, colors denote communities obtained via modularity-based clustering (<a target="_blank" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.68.6623">Brandes et al.</a>, 2008).</p>
							</div>
							<p>Let's take a look at how our simple GCN model (see previous section or <a target="_blank" href="http://arxiv.org/abs/1609.02907">Kipf &amp; Welling</a>, ICLR 2017) works on a well-known graph dataset: Zachary's karate club network (see Figure above).</p>
							<p>We take a 3-layer GCN with randomly initialized weights. Now, even before training the weights, we simply insert the adjacency matrix of the graph and <span class="math inline">\(X = I\)</span> (i.e. the identity matrix, as we don't have any node features) into the model. The 3-layer GCN now performs three propagation steps during the forward pass and effectively convolves the 3rd-order neighborhood of every node (all nodes up to 3 &quot;hops&quot; away). Remarkably, the model produces an embedding of these nodes that closely resembles the community-structure of the graph (see Figure below). Remember that we have initialized the weights completely at random and have not yet performed any training updates (so far)!</p>
							<div class="figure">
							<img src="images/karate_emb.png" alt="GCN embedding (with random weights) for nodes in the karate club network." />
							<p class="caption">GCN embedding (with random weights) for nodes in the karate club network.</p>
							</div>
							<p>This might seem somewhat surprising. A recent paper on a model called DeepWalk (<a target="_blank" href="https://arxiv.org/abs/1403.6652">Perozzi et al.</a>, KDD 2014) showed that they can learn a very similar embedding in a complicated unsupervised training procedure. How is it possible to get such an embedding more or less &quot;for free&quot; using our simple untrained GCN model?</p>
							<p>We can shed some light on this by interpreting the GCN model as a generalized, differentiable version of the well-known Weisfeiler-Lehman algorithm on graphs. The (1-dimensional) Weisfeiler-Lehman algorithm works as follows<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a>:</p>
							<p>For all nodes <span class="math inline">\(v_i\in \mathcal{G}\)</span>:</p>
							<ul>
							<li>Get features<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a> <span class="math inline">\(\{h_{v_j}\}\)</span> of neighboring nodes <span class="math inline">\(\{v_j\}\)</span></li>
							<li>Update node feature <span class="math inline">\(h_{v_i} \leftarrow \text{hash}\left(\sum_j h_{v_j}\right)\)</span>, where <span class="math inline">\(\text{hash}(\cdot)\)</span> is (ideally) an injective hash function</li>
							</ul>
							<p>Repeat for <span class="math inline">\(k\)</span> steps or until convergence.</p>
							<p>In practice, the Weisfeiler-Lehman algorithm assigns a unique set of features for <em>most</em> graphs. This means that every node is assigned a feature that uniquely describes its role in the graph. Exceptions are highly regular graphs like grids, chains, etc. For most irregular graphs, this feature assignment can be used as a check for graph isomorphism (i.e. whether two graphs are identical, up to a permutation of the nodes).</p>
							<p>Going back to our Graph Convolutional layer-wise propagation rule (now in vector form):</p>
							<p><span class="math display">\[h^{(l+1)}_{v_i} = \sigma \left( \sum_{j} \frac{1}{c_{ij}}h^{(l)}_{v_j}W^{(l)} \right) \, ,\]</span></p>
							<p>where <span class="math inline">\(j\)</span> indexes the neighboring nodes of <span class="math inline">\(v_i\)</span>. <span class="math inline">\(c_{ij}\)</span> is a normalization constant for the edge <span class="math inline">\((v_i,v_j)\)</span> which originates from using the symmetrically normalized adjacency matrix <span class="math inline">\(D^{-\frac{1}{2}}AD^{-\frac{1}{2}}\)</span> in our GCN model. We now see that this propagation rule can be interpreted as a differentiable and parameterized (with <span class="math inline">\(W^{(l)}\)</span>) variant of the hash function used in the original Weisfeiler-Lehman algorithm. If we now choose an appropriate non-linearity and initialize the random weight matrix such that it is orthogonal (or e.g. using the initialization from <a target="_blank" href="http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf">Glorot &amp; Bengio</a>, AISTATS 2010), this update rule becomes stable in practice (also thanks to the normalization with <span class="math inline">\(c_{ij}\)</span>). And we make the remarkable observation that we get meaningful smooth embeddings where we can interpret distance as (dis-)similarity of local graph structures!</p>
							<h4 id="gcns-part-iv-semi-supervised-learning">GCNs Part IV: Semi-supervised learning</h4>
							<p>Since everything in our model is differentiable and parameterized, we can add some labels, train the model and observe how the embeddings react. We can use the semi-supervised learning algorithm for GCNs introduced in <a target="_blank" href="http://arxiv.org/abs/1609.02907">Kipf &amp; Welling</a> (ICLR 2017). We simply label one node per class/community (highlighted nodes in the video below) and start training for a couple of iterations<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a>:</p>
							<div class="figure">
							<div class="video"><video width='640' preload='auto' muted controls poster='images/video_.png'>
								<source src='images/video.mp4' type='video/mp4; codecs="avc1.42E01E, mp4a.40.2"'/></video>
							</div>
							<p class="caption">Semi-supervised classification with GCNs: Latent space dynamics for 300 training iterations with a single label per class. Labeled nodes are highlighted.</p>
							</div>
							<p>Note that the model directly produces a 2-dimensional latent space which we can immediately visualize. We observe that the 3-layer GCN model manages to linearly separate the communities, given only one labeled example per class. This is a somewhat remarkable result, given that the model received no feature description of the nodes. At the same time, initial node features <em>could</em> be provided, which is exactly what we do in the experiments described in our paper (<a target="_blank" href="http://arxiv.org/abs/1609.02907">Kipf &amp; Welling</a>, ICLR 2017) to achieve state-of-the-art classification results on a number of graph datasets.</p>
							<h3 id="conclusion">Conclusion</h3>
							<p>Research on this topic is just getting started. The past several months have seen exciting developments, but we have probably only scratched the surface of these types of models so far. It remains to be seen how neural networks on graphs can be further taylored to specific types of problems, like, e.g., learning on directed or relational graphs, and how one can use learned graph embeddings for further tasks down the line, etc. This list is by no means exhaustive and I expect further interesting applications and extensions to pop up in the near future. Let me know in the comments below if you have some exciting ideas or questions to share!</p>
							<h5 id="thanks-to-the-following-people">Thanks to the following people:</h5>
							<p>Max Welling, Taco Cohen, Chris Louizos and Karen Ullrich (for many discussions and feedback both on the paper and this blog post). Also I'd like to thank Ferenc Huszar for highlighting some drawbacks of these kinds of models.</p>
							<h5 id="a-note-on-completeness">A note on completeness</h5>
							<p>This blog post constitutes by no means an exhaustive review of the field of neural networks on graphs. I have left out a number of both recent and older papers to make this post more readable and to give it a coherent story line. The papers that I mentioned here will nonetheless serve as a good start if you want to dive deeper into this topic and get a complete overview of what is around and what has been tried so far.</p>
							<h5 id="citation">Citation</h5>
							<p>If you want to use some of this in your own work, you can cite our <a target="_blank" href="http://arxiv.org/abs/1609.02907">paper</a> on Graph Convolutional Networks:</p>
<pre><code>@article{kipf2016semi,
  title={Semi-Supervised Classification with Graph Convolutional Networks},
  author={Kipf, Thomas N and Welling, Max},
  journal={arXiv preprint arXiv:1609.02907},
  year={2016}
}</code></pre>
							<h5 id="source-code">Source code</h5>
							<p>We have released the code for Graph Convolutional Networks on GitHub: <a target="_blank" href="https://github.com/tkipf/gcn">https://github.com/tkipf/gcn</a>.</p>
							<p>You can follow me on <a target="_blank" href="https://twitter.com/thomaskipf">Twitter</a> for future updates.</p>
							<div class="social_buttons">
							<span class="twitter_button"><a href="https://twitter.com/share" class="twitter-share-button" data-text="How powerful are Graph Convolutional Networks? - An introduction to neural networks on graphs" data-url="http://tkipf.github.io/graph-convolutional-networks/" data-via="thomaskipf" data-lang="en" data-show-count="false">Tweet</a> <a href="http://leadstories.com/opensharecount" target="_blank" class="osc-counter" data-dir="left" data-url="tkipf.github.io/graph-convolutional-networks/" title="Powered by Lead Stories' OpenShareCount">0</a>
</span>
							<span class="facebook_button"><div class="fb-share-button" data-href="http://tkipf.github.io/graph-convolutional-networks/" data-layout="button_count" data-size="small" data-mobile-iframe="false"><a class="fb-xfbml-parse-ignore" target="_blank" href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Ftkipf.github.io%graph-convolutional-networks%2F&amp;src=sdkpreparse">Share</a></div></span>
							</div>
							<hr />
							<h6 id="the-issue-with-regular-graphs">The issue with regular graphs</h6>
							<p class="footnotes">In the following, I will briefly comment on the statements made in <a target="_blank" href="http://www.inference.vc/how-powerful-are-graph-convolutions-review-of-kipf-welling-2016-2/">How powerful are Graph Convolutions?</a>, a recent blog post by Ferenc Huszar that provides a slightly negative view on some of the models discussed here. Ferenc considers the special case of regular graphs. He correctly points out that Graph Convolutional Networks (as introduced in this blog post) reduce to rather trivial operations on regular graphs when compared to models that are specifically designed for this domain (like &quot;classical&quot; 2D CNNs for images). It is indeed important to note that current graph neural network models that apply to arbitrarily structured graphs typically share some form of shortcoming when applied to regular graphs (like grids, chains, fully-connected graphs etc.). A localized spectral treatment (like in <a target="_blank" href="https://arxiv.org/abs/1606.09375">Defferrard et al.</a>, NIPS 2016), for example, reduces to rotationally symmetric filters and can never imitate the operation of a &quot;classical&quot; 2D CNN on a grid (exluding border-effects). In the same way, the Weisfeiler-Lehman algorithm will not converge on regular graphs. What this tells us, is that we should probably look beyond regular grids when trying to evaluate the usefulness of a specific graph neural network model, as there are specific trade-offs that have to be made when designing such models for arbitary graphs (yet it is of course important to make people aware of these trade-offs) - that is, unless we can come up with a universally powerful model at some point, of course.</p>
							<div class="footnotes">
							<hr />
							<ol>
							<li id="fn1"><p>A spectral graph convolution is defined as the multiplication of a signal with a filter in the Fourier space of a graph. A graph Fourier transform is defined as the multiplication of a graph signal <span class="math inline">\(X\)</span> (i.e. feature vectors for every node) with the eigenvector matrix <span class="math inline">\(U\)</span> of the graph Laplacian <span class="math inline">\(L\)</span>. The (normalized) graph Laplacian can be easily computed from the symmetrically normalized graph adjacency matrix <span class="math inline">\(\tilde{A}\)</span>: <span class="math inline">\(L = I - \tilde{A}\)</span>.<a href="#fnref1">↩</a></p></li>
							<li id="fn2"><p>Using a spectral approach comes at a price: Filters have to be defined in Fourier space and a graph Fourier transform is expensive to compute (it requires multiplication of node features with the eigenvector matrix of the graph Laplacian, which is a <span class="math inline">\(O(N^2)\)</span> operation for a graph with <span class="math inline">\(N\)</span> nodes; computing the eigenvector matrix in the first place is even more expensive).<a href="#fnref2">↩</a></p></li>
							<li id="fn3"><p>I'm simplifying things here. For an extensive mathematical discussion of the Weisfeiler-Lehman algorithm, have a look at the paper by <a target="_blank" href="https://arxiv.org/abs/1101.5211">Douglas</a> (2011).<a href="#fnref3">↩</a></p></li>
							<li id="fn4"><p>Node features for the Weisfeiler-Lehman algorithm are typically chosen as scalar integers and are often referred to as colors.<a href="#fnref4">↩</a></p></li>
							<li id="fn5"><p>As we don't anneal the learning rate in this example, things become quite &quot;jiggly&quot; once the model has converged to a good solution.<a href="#fnref5">↩</a></p></li>
							</ol>
							</div>

							<div id="disqus_thread"></div>
<script>
var disqus_config = function () {
    this.page.url = 'http://tkipf.github.io/graph-convolutional-networks/';  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = 'graph-convolutional-networks'; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
};
(function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = '//tkipf-github-io.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
})();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>



							<!--
							<div class="figure">
							<div class="video"><video width='640' preload='auto' muted controls poster='images/video_.png'>
								<source src='images/video.mp4' type='video/mp4; codecs="avc1.42E01E, mp4a.40.2"'/></video>
							</div>
							<p class="caption">Semi-supervised classification with GCNs: Latent space dynamics for 300 training iterations with a single label per class. Labeled nodes are highlighted.</p>
							</div>
						-->
						</div>
				</section>


		<!-- Footer -->
			<footer id="footer">
				<div class="container">
					<h2>Get in touch</h2>
					<ul class="icons">
						<li>
							<a href="https://scholar.google.com/citations?user=83HL5FwAAAAJ" class="icon fa-graduation-cap"></a>
						</li>
						<li>
							<a href="https://github.com/tkipf" class="icon fa-github"></a>
						</li>
						<li>
							<a href="https://twitter.com/thomaskipf" class="icon fa-twitter"></a>
						</li>
						<li>
							<a href="https://www.linkedin.com/in/thomas-kipf-6b260410a" class="icon fa-linkedin"></a>
						</li>
					</ul>
					<ul class="copyright">
						<li>&copy; 2020 Thomas Kipf</li>
						<li>Design: <a href="http://templated.co">TEMPLATED</a></li>
					</ul>
				</div>
			</footer>

	</body>
</html>
